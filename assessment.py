# -*- coding: utf-8 -*-
"""extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p1wNZ2VLWf_L54jTDojN4NYmTw2QiILJ

# **Blackcoffer Assessment**

**Importing the Libraries**
"""

import os
import requests
import pandas as pd
from bs4 import BeautifulSoup

import os
import re
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords

"""**Reading the Input.xlsx excel file**"""

file_path = 'Input.xlsx'
df = pd.read_excel(file_path)

"""**Creating a directory to store the extracted text**"""

output_dir = 'file_output'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

"""**Funtion to only extract the article content while ignoring footers, headers etc.**"""

def extract_article_text(url):
    try:
        response = requests.get(url)
        parsed_content = BeautifulSoup(response.content, 'html.parser')


        article_texts = []

        # search for the <article> tag
        article_tag = parsed_content.find('article')
        if article_tag:
            for unwanted in article_tag(['header', 'footer', 'aside', 'nav', 'form', 'script']):
                unwanted.extract()
            article_texts.append(article_tag.get_text(separator="\n", strip=True))

        # search for <div> tags with the classes content and post-content
        div_content = parsed_content.find_all('div', class_='content')
        div_post_content = parsed_content.find_all('div', class_='post-content')

        # appending the contetent from <div> tags along with removing the unecesary tags like <header>, <footer>, <nav> etc
        for div in div_content:
            for unwanted in div(['header', 'footer', 'aside', 'nav', 'form', 'script']):
                unwanted.extract()
            article_texts.append(div.get_text(separator="\n", strip=True))


        for div in div_post_content:
            for unwanted in div(['header', 'footer', 'aside', 'nav', 'form', 'script']):
                unwanted.extract()
            article_texts.append(div.get_text(separator="\n", strip=True))


        if article_texts:
            return "\n".join(article_texts)
        else:
            return "No article content found in this page."

    except Exception as e:
        return f"Error: {e}"

"""**Iteration over all the urls in the Input.xlsx**"""

for index, row in df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']


    article_text = extract_article_text(url)


    output_file_path = os.path.join(output_dir, f"{url_id}.txt")
    with open(output_file_path, 'w', encoding='utf-8') as file:
        file.write(article_text)

print("Article text extraction completed!")

"""**Getting the StopWords , positive and negative words from the given data**"""

nltk.download('punkt')
nltk.download('stopwords')


stopwords_folder = 'StopWords'
stopwords_files = [
    'StopWords_Auditor.txt', 'StopWords_Currencies.txt', 'StopWords_DatesandNumbers.txt',
    'StopWords_Generic.txt', 'StopWords_GenericLong.txt', 'StopWords_Geographic.txt',
    'StopWords_Names.txt'
]

stop_words = set(stopwords.words('english'))

for file in stopwords_files:
    file_path = os.path.join(stopwords_folder, file)
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        custom_stopwords = f.read().splitlines()
        stop_words.update(custom_stopwords)

with open('MasterDictionary/positive-words.txt', 'r', encoding='utf-8', errors='ignore') as f:
    positive_words = set(f.read().splitlines())

with open('MasterDictionary/negative-words.txt', 'r', encoding='utf-8', errors='ignore') as f:
    negative_words = set(f.read().splitlines())

"""**Function to determine the number of syllables in the given words**"""

def syllable_count(word):
    vowels = "aeiou"
    count = 0
    word = word.lower()
    if word[0] in vowels:
        count += 1
    for index in range(1, len(word)):
        if word[index] in vowels and word[index - 1] not in vowels:
            count += 1
    if word.endswith("es") or word.endswith("ed"):
        count -= 1
    if count == 0:
        count += 1
    return count

"""**Function to determine the variables mentioned in the assignment**"""

def analyze_text(text):

    words = word_tokenize(text)
    sentences = sent_tokenize(text)

    cleaned_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]


    positive_score = sum(1 for word in cleaned_words if word in positive_words)

    negative_score = sum(1 for word in cleaned_words if word in negative_words)

    word_count = len(cleaned_words)

    sentence_count = len(sentences)

    complex_word_count = sum(1 for word in cleaned_words if syllable_count(word) > 2)

    syllable_per_word = sum(syllable_count(word) for word in cleaned_words) / word_count if word_count else 0

    average_word_length = sum(len(word) for word in cleaned_words) / word_count if word_count else 0


    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)

    subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)


    average_sentence_length = word_count / sentence_count if sentence_count else 0

    percentage_complex_words = complex_word_count / word_count if word_count else 0

    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)


    personal_pronouns = len(re.findall(r'\b(I|we|my|ours|us)\b', text, re.I))


    return {
        "POSITIVE SCORE": positive_score,
        "NEGATIVE SCORE": negative_score,
        "POLARITY SCORE": polarity_score,
        "SUBJECTIVITY SCORE": subjectivity_score,
        "AVG SENTENCE LENGTH": average_sentence_length,
        "PERCENTAGE OF COMPLEX WORDS": percentage_complex_words,
        "FOG INDEX": fog_index,
        "AVG NUMBER OF WORDS PER SENTENCE": average_sentence_length,
        "COMPLEX WORD COUNT": complex_word_count,
        "WORD COUNT": word_count,
        "SYLLABLE PER WORD": syllable_per_word,
        "PERSONAL PRONOUNS": personal_pronouns,
        "AVG WORD LENGTH": average_word_length
    }

"""**Extract text from each file and perform analysis, then store it in the excel sheet**"""

output_file = 'Output Data Structure.xlsx'
output_df = pd.read_excel(output_file)


file_output_folder = 'file_output'


for index, row in output_df.iterrows():
    url_id = row['URL_ID']


    text_file_path = os.path.join(file_output_folder, f"{url_id}.txt")
    if os.path.exists(text_file_path):
        with open(text_file_path, 'r', encoding='utf-8') as f:
            text = f.read()


        analysis_result = analyze_text(text)


        for column, value in analysis_result.items():
            output_df.at[index, column] = value
    else:
        print(f"File {url_id}.txt not found!")


output_df.to_excel(output_file, index=False)

print("Text analysis completed and written to Output Data Structure.xlsx")